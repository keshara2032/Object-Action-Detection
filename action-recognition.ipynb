{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x_set, y_set, seq_len):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # return int(np.ceil(len(self.x) / float(self.seq_len)))\n",
    "        return int((len(self.x) ))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # start_idx = idx * self.seq_len\n",
    "        # end_idx = (idx + 1) * self.seq_len\n",
    "        \n",
    "        # sliding window\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.seq_len\n",
    "\n",
    "        batch_x = self.x[start_idx+1:end_idx+1]\n",
    "        batch_y = self.y[start_idx:end_idx+1]\n",
    "\n",
    "        # Count the occurrences of each row\n",
    "        unique_rows, counts = np.unique(batch_y, axis=0, return_counts=True)\n",
    "        \n",
    "        # print(\"index:\",idx,\"ybatch:\",batch_y)\n",
    "\n",
    "        # Get the index of the row with the highest count\n",
    "        most_common_row_index = np.argmax(counts)\n",
    "\n",
    "        # Get the most common row\n",
    "        most_common_row = unique_rows[most_common_row_index]\n",
    "        \n",
    "        \n",
    "        batch_y_bin = most_common_row\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        batch_x = torch.from_numpy(batch_x)\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        batch_y_bin = torch.from_numpy(batch_y_bin)\n",
    "\n",
    "        # Pad sequences to ensure they have the same length within the batch\n",
    "        pad_len = self.seq_len - batch_x.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_shape = (pad_len,) + batch_x.shape[1:]\n",
    "            pad_shape_y = (pad_len,) + batch_y.shape[1:]\n",
    "\n",
    "            batch_x = torch.cat([batch_x, torch.zeros(pad_shape)], dim=0)\n",
    "            batch_y = torch.cat([batch_y, torch.zeros(pad_shape_y)], dim=0)\n",
    "\n",
    "        return batch_x, batch_y, batch_y_bin\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        indices = np.arange(len(self.x))\n",
    "        np.random.shuffle(indices)\n",
    "        self.x = self.x[indices]\n",
    "        self.y = self.y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Trials:  23\n",
      "['I0' 'I1' 'I2']\n",
      "0 torch.Size([30, 10, 126]) torch.Size([30, 11, 3]) torch.Size([30, 3])\n"
     ]
    }
   ],
   "source": [
    "# dataloaders\n",
    "def generate_data(features, batch_size, seq_len):    \n",
    "    \n",
    "    csv_path = './processed_data'\n",
    "    \n",
    "    csv_files = glob.iglob(csv_path + \"/**/*.csv\", recursive=True)\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "\n",
    "    for file in csv_files:\n",
    "        df_list.append(pd.read_csv(file))\n",
    "            \n",
    "\n",
    "    print('All Trials: ',len(df_list))\n",
    "    \n",
    "#     # Concatenate all DataFrames\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "    df_labels= df.pop('label')\n",
    "    df_features = df\n",
    "    \n",
    "    # print(df_labels)\n",
    "    # df_features = df_features['landmarks'].apply(pd.Series)\n",
    "\n",
    "\n",
    "#     test_labels= test_df.pop('label')\n",
    "#     test_features = test_df\n",
    "\n",
    "\n",
    "    all_interventions = [\"I0\", 'I1', 'I2']\n",
    "    lb.fit(all_interventions)\n",
    "    # lb.fit(df_labels)\n",
    "    \n",
    "    print(lb.classes_)\n",
    "    \n",
    "\n",
    "\n",
    "    df_labels = lb.transform(df_labels)\n",
    "#     test_labels = lb.transform(test_labels)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_features.to_numpy(), df_labels, test_size=0.3)\n",
    "    \n",
    "    # np.savetxt(\"./processed_data/trainx.csv\", train_x, delimiter=\",\")\n",
    "    # np.savetxt(\"./processed_data/testx.csv\", train_y, delimiter=\",\")\n",
    "    # np.savetxt(\"./processed_data/trainy.csv\", train_y, delimiter=\",\")\n",
    "    # np.savetxt(\"./processed_data/testy.csv\", test_y, delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(train_x, train_y, seq_len)\n",
    "    test_dataset = TimeSeriesDataset(test_x, test_y, seq_len)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    " \n",
    "    \n",
    "    \n",
    "features = 38\n",
    "batch_size = 30\n",
    "seq_len = 10\n",
    "output_dim = 14\n",
    "\n",
    "train_dataloader, test_dataloader = generate_data(features, batch_size, seq_len)\n",
    "\n",
    "for idx,batch in enumerate(test_dataloader):\n",
    "    \n",
    "    print(idx, batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling1D(nn.Module):\n",
    "\n",
    "    def __init__(self, data_format='channels_last'):\n",
    "        super(GlobalMaxPooling1D, self).__init__()\n",
    "        self.data_format = data_format\n",
    "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.max(input, axis=self.step_axis).values\n",
    "    \n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers,  dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "  \n",
    "        x = self.transformer(x)\n",
    "        \n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trans-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransLSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, hidden_dim, layer_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.lstm = LSTMModel(d_model, hidden_dim, layer_dim, output_dim)\n",
    "        \n",
    "        self.max_pool = GlobalMaxPooling1D()\n",
    "        self.fc = nn.Linear(input_dim, d_model)\n",
    "        self.out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "    \n",
    "        x = self.lstm(x)\n",
    "        \n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_tlstm(dataloader,model,optimizer,scheduler,criterion, epochs):\n",
    "    \n",
    "    model.train()\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x, y, y_seq = batch\n",
    "            x = x.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_seq = y_seq.to(torch.float32)\n",
    "            \n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_seq = y_seq.cuda()\n",
    "            \n",
    "            \n",
    "            y_pred = model(x)\n",
    "\n",
    "#             print(y_pred.shape, y_seq.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y_seq)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_loop(dataloader,model,optimizer,criterion, epochs):\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, y_seq = batch\n",
    "            x = x.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_seq = y_seq.to(torch.float32)\n",
    "            \n",
    "            \n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_seq = y_seq.cuda()\n",
    "        \n",
    "            y_pred = model(x)\n",
    "\n",
    "            # print(y_pred.shape, y_seq.shape)\n",
    "            \n",
    "            loss = criterion(y_pred, y_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(dataloader, model,criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_accuracy = []\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        x, y, y_seq = batch\n",
    "        x = x.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "        y_seq = y_seq.to(torch.float32)\n",
    "        \n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_seq = y_seq.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        total_inputs = 0\n",
    "        true_pred = []\n",
    "        \n",
    "        \n",
    "        for idx,y in enumerate(y_pred):\n",
    "            \n",
    "            total_inputs += 1\n",
    "            \n",
    "            output_argmax = torch.argmax(y)\n",
    "            gt_argmax = torch.argmax(y_seq[idx])\n",
    "        \n",
    "            if(output_argmax == gt_argmax):\n",
    "                true_pred.append(output_argmax)\n",
    "                \n",
    "            accuracy = len(true_pred)/total_inputs\n",
    "            \n",
    "            # print(\"Accuracy: \",accuracy)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "            \n",
    "                \n",
    "        loss = criterion(y_pred, y_seq)\n",
    "        # print(i, \"Loss: \", loss)\n",
    "        \n",
    "    avg_accuracy = np.average(total_accuracy)\n",
    "    print(\"Average accuracy: \", avg_accuracy)\n",
    "    return avg_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model_tlstm(d_model, nhead, num_layers,input_dim,output_dim, hidden_dim, layer_dim, lr, iterations_per_epoch ):\n",
    "\n",
    "    model = TransLSTMModel(input_dim=input_dim, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers, hidden_dim=hidden_dim, layer_dim=layer_dim)\n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # adam\n",
    "    \n",
    "    sched = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer,sched, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model(device, d_model, nhead, num_layers, features, output_dim, lr):\n",
    "    model = TransformerModel(input_dim=features, output_dim=output_dim, d_model=d_model, nhead=nhead, num_layers=num_layers)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 3\n",
      "GPU is available\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 5\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "iterations_per_epoch = 500\n",
    "lr = 1e-4\n",
    "\n",
    "# lstm\n",
    "hidden_dim = 512\n",
    "layer_dim = 2\n",
    "seq_dim = 128\n",
    "\n",
    "# transformer\n",
    "d_model = 128\n",
    "nhead=8\n",
    "num_layers=4\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "features = batch[0].shape[-1]\n",
    "output_dim = batch[2].shape[-1]\n",
    "\n",
    "print(features, output_dim)\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Trials:  23\n",
      "['I0' 'I1' 'I2']\n",
      "Epoch 1, Loss: 0.746747\n",
      "Epoch 2, Loss: 0.670312\n",
      "Epoch 3, Loss: 0.665763\n",
      "Epoch 4, Loss: 0.652167\n",
      "Epoch 5, Loss: 0.639889\n",
      "Epoch 6, Loss: 0.644612\n",
      "Epoch 7, Loss: 0.615627\n",
      "Epoch 8, Loss: 0.594180\n",
      "Epoch 9, Loss: 0.574692\n",
      "Epoch 10, Loss: 0.556901\n",
      "Epoch 11, Loss: 0.524874\n",
      "Epoch 12, Loss: 0.489345\n",
      "Epoch 13, Loss: 0.469621\n",
      "Epoch 14, Loss: 0.452086\n",
      "Epoch 15, Loss: 0.446329\n",
      "Epoch 16, Loss: 0.437279\n",
      "Epoch 17, Loss: 0.429520\n",
      "Epoch 18, Loss: 0.430132\n",
      "Epoch 19, Loss: 0.474947\n",
      "Epoch 20, Loss: 0.464898\n",
      "Epoch 21, Loss: 0.449642\n",
      "Epoch 22, Loss: 0.431453\n",
      "Epoch 23, Loss: 0.419836\n",
      "Epoch 24, Loss: 0.409354\n",
      "Epoch 25, Loss: 0.392824\n",
      "Epoch 26, Loss: 0.381259\n",
      "Epoch 27, Loss: 0.366344\n",
      "Epoch 28, Loss: 0.352681\n",
      "Epoch 29, Loss: 0.346126\n",
      "Epoch 30, Loss: 0.338470\n",
      "Average accuracy:  0.7680290479328891\n",
      "{'accuracy': 0.7680290479328891}\n"
     ]
    }
   ],
   "source": [
    "model_arch = 'transformer'\n",
    "model_arch = 'translstm'\n",
    "\n",
    "if(model_arch == 'transformer'):\n",
    "    model,optimizer,criterion = initiate_model(device, d_model, nhead, num_layers, features, output_dim, lr)\n",
    "    \n",
    "    train_dataloader, test_dataloader = generate_data(features, batch_size, seq_len)\n",
    "\n",
    "    train_loop(dataloader=train_dataloader, model=model, optimizer=optimizer, criterion=criterion, epochs=epochs)\n",
    "\n",
    "    acc = evaluation_loop(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "\n",
    "    subject_accuracy = {'accuracy':acc}\n",
    "    print(subject_accuracy)\n",
    "    \n",
    "else:\n",
    "    model,optimizer,scheduler,criterion = initiate_model_tlstm(d_model, nhead, num_layers,features,output_dim, hidden_dim, layer_dim, lr,  iterations_per_epoch=iterations_per_epoch)\n",
    "    \n",
    "    train_dataloader, test_dataloader = generate_data(features, batch_size, seq_len)\n",
    "\n",
    "    train_loop_tlstm(dataloader=train_dataloader, model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion, epochs=epochs)\n",
    "\n",
    "    acc = evaluation_loop(dataloader=test_dataloader, model=model, criterion=criterion)\n",
    "\n",
    "    subject_accuracy = {'accuracy':acc}\n",
    "    print(subject_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
